{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "454f8bb0-4201-4a3a-b1bb-dcc90b00ec5f",
   "metadata": {},
   "source": [
    "# Diagnostics\n",
    "\n",
    "Some commands to see what's going on in the cluster\n",
    "\n",
    "## Links\n",
    "\n",
    "* [https://kubernetes.io/docs/concepts/architecture/](https://kubernetes.io/docs/concepts/architecture/)\n",
    "* [https://kubernetes.io/docs/tasks/debug/debug-cluster/](https://kubernetes.io/docs/tasks/debug/debug-cluster/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "98355322-d941-46d0-8b65-1d4f4c4ce283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all:\n",
      "  hosts:\n",
      "    node1:\n",
      "      ansible_host: xxx.xxx.235.216\n",
      "      ip: xxx.xxx.235.216\n",
      "      access_ip: xxx.xxx.235.216\n",
      "    node2:\n",
      "      ansible_host: xxx.xxx.195.197\n",
      "      ip: xxx.xxx.195.197\n",
      "      access_ip: xxx.xxx.195.197\n",
      "    node3:\n",
      "      ansible_host: xxx.xxx.195.230\n",
      "      ip: xxx.xxx.195.230\n",
      "      access_ip: xxx.xxx.195.230\n",
      "  children:\n",
      "    kube_control_plane:\n",
      "      hosts:\n",
      "        node1:\n",
      "    kube_node:\n",
      "      hosts:\n",
      "        node2:\n",
      "        node3:\n",
      "    etcd:\n",
      "      hosts:\n",
      "        node1:\n",
      "    k8s_cluster:\n",
      "      children:\n",
      "        kube_control_plane:\n",
      "        kube_node:\n",
      "    calico_rr:\n",
      "      hosts: {}\n"
     ]
    }
   ],
   "source": [
    "# Print the hosts.yml used for this cluser:\n",
    "! cat mycluster/hosts.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d1f78b9-ff2c-42c8-a979-363daa1f020f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[0;32mKubernetes control plane\u001B[0m is running at \u001B[0;33mhttps://xxx.xxx.235.216:6443\u001B[0m\n",
      "\n",
      "To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
     ]
    }
   ],
   "source": [
    "! kubectl cluster-info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608b5c96-0ce7-4d66-bcbf-a0a88af53f01",
   "metadata": {},
   "source": [
    "**Warning** The output of `kubectl cluster-info dump` is huge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b4cc7cf-a1c7-4578-9c8a-6fa16171ca8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAMESPACE     NAME                                           READY   STATUS    RESTARTS      AGE\n",
      "kube-system   pod/calico-kube-controllers-6dd874f784-p7pt7   1/1     Running   0             24m\n",
      "kube-system   pod/calico-node-5hvhv                          1/1     Running   0             25m\n",
      "kube-system   pod/calico-node-mxmmd                          1/1     Running   0             25m\n",
      "kube-system   pod/calico-node-wfnlr                          1/1     Running   0             25m\n",
      "kube-system   pod/coredns-76b4fb4578-gf6rx                   1/1     Running   0             23m\n",
      "kube-system   pod/coredns-76b4fb4578-nhwzg                   1/1     Running   0             24m\n",
      "kube-system   pod/dns-autoscaler-7979fb6659-z68nd            1/1     Running   0             24m\n",
      "kube-system   pod/kube-apiserver-node1                       1/1     Running   1             27m\n",
      "kube-system   pod/kube-controller-manager-node1              1/1     Running   2 (23m ago)   27m\n",
      "kube-system   pod/kube-proxy-lqgm2                           1/1     Running   0             25m\n",
      "kube-system   pod/kube-proxy-r2xpb                           1/1     Running   0             25m\n",
      "kube-system   pod/kube-proxy-vp8gg                           1/1     Running   0             25m\n",
      "kube-system   pod/kube-scheduler-node1                       1/1     Running   2 (23m ago)   27m\n",
      "kube-system   pod/nginx-proxy-node2                          1/1     Running   0             25m\n",
      "kube-system   pod/nginx-proxy-node3                          1/1     Running   0             25m\n",
      "kube-system   pod/nodelocaldns-jnl8q                         0/1     Pending   0             24m\n",
      "kube-system   pod/nodelocaldns-nt6t9                         1/1     Running   0             24m\n",
      "kube-system   pod/nodelocaldns-ps6rs                         1/1     Running   0             24m\n",
      "\n",
      "NAMESPACE     NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                  AGE\n",
      "default       service/kubernetes   ClusterIP   10.233.0.1   <none>        443/TCP                  27m\n",
      "kube-system   service/coredns      ClusterIP   10.233.0.3   <none>        53/UDP,53/TCP,9153/TCP   24m\n",
      "\n",
      "NAMESPACE     NAME                          DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE\n",
      "kube-system   daemonset.apps/calico-node    3         3         3       3            3           kubernetes.io/os=linux   25m\n",
      "kube-system   daemonset.apps/kube-proxy     3         3         3       3            3           kubernetes.io/os=linux   27m\n",
      "kube-system   daemonset.apps/nodelocaldns   3         3         2       3            2           kubernetes.io/os=linux   24m\n",
      "\n",
      "NAMESPACE     NAME                                      READY   UP-TO-DATE   AVAILABLE   AGE\n",
      "kube-system   deployment.apps/calico-kube-controllers   1/1     1            1           24m\n",
      "kube-system   deployment.apps/coredns                   2/2     2            2           24m\n",
      "kube-system   deployment.apps/dns-autoscaler            1/1     1            1           24m\n",
      "\n",
      "NAMESPACE     NAME                                                 DESIRED   CURRENT   READY   AGE\n",
      "kube-system   replicaset.apps/calico-kube-controllers-6dd874f784   1         1         1       24m\n",
      "kube-system   replicaset.apps/coredns-76b4fb4578                   2         2         2       24m\n",
      "kube-system   replicaset.apps/dns-autoscaler-7979fb6659            1         1         1       24m\n"
     ]
    }
   ],
   "source": [
    "! kubectl get all --all-namespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7ff6cdb-fa71-411f-80a5-a79f4468703e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME    STATUS   ROLES                  AGE   VERSION\n",
      "node1   Ready    control-plane,master   39m   v1.23.7\n",
      "node2   Ready    <none>                 38m   v1.23.7\n",
      "node3   Ready    <none>                 38m   v1.23.7\n"
     ]
    }
   ],
   "source": [
    "! kubectl get nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15888db9-3971-4632-a1bc-a0c3a109e7c7",
   "metadata": {},
   "source": [
    "**Overview of node status** (https://kubernetes.io/docs/concepts/architecture/nodes/#node-status)[https://kubernetes.io/docs/concepts/architecture/nodes/#node-status}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf3d8765-7c18-454f-b1f6-2fe242802ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:               node2\n",
      "Roles:              <none>\n",
      "Labels:             beta.kubernetes.io/arch=amd64\n",
      "                    beta.kubernetes.io/os=linux\n",
      "                    kubernetes.io/arch=amd64\n",
      "                    kubernetes.io/hostname=node2\n",
      "                    kubernetes.io/os=linux\n",
      "Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/containerd/containerd.sock\n",
      "                    node.alpha.kubernetes.io/ttl: 0\n",
      "                    projectcalico.org/IPv4Address: xxx.xxx.195.197/32\n",
      "                    projectcalico.org/IPv4VXLANTunnelAddr: 10.233.96.0\n",
      "                    volumes.kubernetes.io/controller-managed-attach-detach: true\n",
      "CreationTimestamp:  Wed, 08 Jun 2022 06:32:22 +0000\n",
      "Taints:             <none>\n",
      "Unschedulable:      false\n",
      "Lease:\n",
      "  HolderIdentity:  node2\n",
      "  AcquireTime:     <unset>\n",
      "  RenewTime:       Wed, 08 Jun 2022 09:34:46 +0000\n",
      "Conditions:\n",
      "  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n",
      "  ----                 ------  -----------------                 ------------------                ------                       -------\n",
      "  NetworkUnavailable   False   Wed, 08 Jun 2022 06:33:11 +0000   Wed, 08 Jun 2022 06:33:11 +0000   CalicoIsUp                   Calico is running on this node\n",
      "  MemoryPressure       False   Wed, 08 Jun 2022 09:34:45 +0000   Wed, 08 Jun 2022 06:32:22 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n",
      "  DiskPressure         False   Wed, 08 Jun 2022 09:34:45 +0000   Wed, 08 Jun 2022 06:32:22 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n",
      "  PIDPressure          False   Wed, 08 Jun 2022 09:34:45 +0000   Wed, 08 Jun 2022 06:32:22 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n",
      "  Ready                True    Wed, 08 Jun 2022 09:34:45 +0000   Wed, 08 Jun 2022 06:33:22 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\n",
      "Addresses:\n",
      "  InternalIP:  xxx.xxx.195.197\n",
      "  Hostname:    node2\n",
      "Capacity:\n",
      "  cpu:                2\n",
      "  ephemeral-storage:  39062284Ki\n",
      "  hugepages-1Gi:      0\n",
      "  hugepages-2Mi:      0\n",
      "  memory:             3927728Ki\n",
      "  pods:               110\n",
      "Allocatable:\n",
      "  cpu:                1900m\n",
      "  ephemeral-storage:  35999800875\n",
      "  hugepages-1Gi:      0\n",
      "  hugepages-2Mi:      0\n",
      "  memory:             3563184Ki\n",
      "  pods:               110\n",
      "System Info:\n",
      "  Machine ID:                 9a519faca3e143b7805fa9999187e886\n",
      "  System UUID:                9a519fac-a3e1-43b7-805f-a9999187e886\n",
      "  Boot ID:                    19a4df44-2558-4171-9a8d-340d03598cdd\n",
      "  Kernel Version:             5.4.0-110-generic\n",
      "  OS Image:                   Ubuntu 20.04.4 LTS\n",
      "  Operating System:           linux\n",
      "  Architecture:               amd64\n",
      "  Container Runtime Version:  containerd://1.6.4\n",
      "  Kubelet Version:            v1.23.7\n",
      "  Kube-Proxy Version:         v1.23.7\n",
      "PodCIDR:                      10.233.66.0/24\n",
      "PodCIDRs:                     10.233.66.0/24\n",
      "Non-terminated Pods:          (6 in total)\n",
      "  Namespace                   Name                                        CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n",
      "  ---------                   ----                                        ------------  ----------  ---------------  -------------  ---\n",
      "  kube-system                 calico-kube-controllers-6dd874f784-vgt2p    30m (1%)      1 (52%)     64M (1%)         256M (7%)      3h1m\n",
      "  kube-system                 calico-node-94ldz                           150m (7%)     300m (15%)  64M (1%)         500M (13%)     3h1m\n",
      "  kube-system                 coredns-76b4fb4578-fvkgt                    100m (5%)     0 (0%)      70Mi (2%)        170Mi (4%)     3h\n",
      "  kube-system                 kube-proxy-xn9rx                            0 (0%)        0 (0%)      0 (0%)           0 (0%)         3h2m\n",
      "  kube-system                 nginx-proxy-node2                           25m (1%)      0 (0%)      32M (0%)         0 (0%)         3h2m\n",
      "  kube-system                 nodelocaldns-f29kg                          100m (5%)     0 (0%)      70Mi (2%)        170Mi (4%)     3h\n",
      "Allocated resources:\n",
      "  (Total limits may be over 100 percent, i.e., overcommitted.)\n",
      "  Resource           Requests        Limits\n",
      "  --------           --------        ------\n",
      "  cpu                405m (21%)      1300m (68%)\n",
      "  memory             306800640 (8%)  1112515840 (30%)\n",
      "  ephemeral-storage  0 (0%)          0 (0%)\n",
      "  hugepages-1Gi      0 (0%)          0 (0%)\n",
      "  hugepages-2Mi      0 (0%)          0 (0%)\n",
      "Events:              <none>\n"
     ]
    }
   ],
   "source": [
    "! kubectl describe node node2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ac628b2-43bc-437c-ae58-f05b91f96df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0608 07:39:10.759981   17503 loader.go:372] Config loaded from file:  /root/.kube/config\n",
      "I0608 07:39:10.905631   17503 round_trippers.go:553] GET https://xxx.xxx.235.216:6443/api/v1/namespaces/default/pods?limit=500 200 OK in 107 milliseconds\n",
      "I0608 07:39:10.932693   17503 round_trippers.go:553] GET https://xxx.xxx.235.216:6443/api/v1/namespaces/default/replicationcontrollers?limit=500 200 OK in 26 milliseconds\n",
      "I0608 07:39:10.961948   17503 round_trippers.go:553] GET https://xxx.xxx.235.216:6443/api/v1/namespaces/default/services?limit=500 200 OK in 29 milliseconds\n",
      "I0608 07:39:10.988432   17503 round_trippers.go:553] GET https://xxx.xxx.235.216:6443/apis/apps/v1/namespaces/default/daemonsets?limit=500 200 OK in 25 milliseconds\n",
      "I0608 07:39:11.013283   17503 round_trippers.go:553] GET https://xxx.xxx.235.216:6443/apis/apps/v1/namespaces/default/deployments?limit=500 200 OK in 24 milliseconds\n",
      "I0608 07:39:11.038686   17503 round_trippers.go:553] GET https://xxx.xxx.235.216:6443/apis/apps/v1/namespaces/default/replicasets?limit=500 200 OK in 25 milliseconds\n",
      "I0608 07:39:11.069041   17503 round_trippers.go:553] GET https://xxx.xxx.235.216:6443/apis/apps/v1/namespaces/default/statefulsets?limit=500 200 OK in 29 milliseconds\n",
      "I0608 07:39:11.094674   17503 round_trippers.go:553] GET https://xxx.xxx.235.216:6443/apis/autoscaling/v2/namespaces/default/horizontalpodautoscalers?limit=500 200 OK in 25 milliseconds\n",
      "I0608 07:39:11.119442   17503 round_trippers.go:553] GET https://xxx.xxx.235.216:6443/apis/batch/v1/namespaces/default/cronjobs?limit=500 200 OK in 24 milliseconds\n",
      "I0608 07:39:11.144718   17503 round_trippers.go:553] GET https://xxx.xxx.235.216:6443/apis/batch/v1/namespaces/default/jobs?limit=500 200 OK in 24 milliseconds\n",
      "NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\n",
      "service/kubernetes   ClusterIP   10.233.0.1   <none>        443/TCP   68m\n"
     ]
    }
   ],
   "source": [
    "# What's running in the default namespace ?\n",
    "! kubectl get all -v=6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ee8427-21df-4d9c-9491-a887a00758fe",
   "metadata": {},
   "source": [
    "## Apparently, the kubernetes servcies are packed into a so-called \"slice\". A new concept for me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f17cb2bc-5834-4524-acb7-8cbe08b08239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Permanently added 'xxx.xxx.235.216' (ECDSA) to the list of known hosts.\n",
      "● kubepods.slice - libcontainer container kubepods.slice\n",
      "     Loaded: loaded (/run/systemd/transient/kubepods.slice; transient)\n",
      "  Transient: yes\n",
      "    Drop-In: /run/systemd/transient/kubepods.slice.d\n",
      "             └─50-CPUShares.conf, 50-MemoryLimit.conf, 50-TasksMax.conf\n",
      "     Active: active since Mon 2022-06-06 08:23:30 UTC; 31min ago\n",
      "      Tasks: 95 (limit: 4194304)\n",
      "     Memory: 633.2M (limit: 1.3G)\n",
      "        CPU: 4min 13.578s\n",
      "     CGroup: /kubepods.slice\n",
      "             ├─kubepods-besteffort.slice\n",
      "             │ └─kubepods-besteffort-pod769636f9_4b3c_468c_81b0_a109e8f3563d.slice\n",
      "             │   ├─cri-containerd-13cec8dabe40c4cece0c74c42f19c13c8463bc2887d15691b4c9149a08cb29f3.scope\n",
      "             │   │ └─13900 /usr/local/bin/kube-proxy --config=/var/lib/kube-proxy/config.conf --hostname-override=node1\n",
      "             │   └─cri-containerd-8887f221e20e74962a6202bef261956c22d93d4693c7374d38dc71581164e993.scope\n",
      "             │     └─13861 /pause\n",
      "             └─kubepods-burstable.slice\n",
      "               ├─kubepods-burstable-pod404ded66_90ac_4d34_8a8f_4b343e3c1834.slice\n",
      "               │ ├─cri-containerd-2669e602dccad8f36ee7e9f9283e990cdbd063d2b46085cd9a28b223c24f1bcb.scope\n",
      "               │ │ └─17556 /pause\n",
      "               │ └─cri-containerd-7c3bffbbc6596b31bb45025d83f54a662e5d4bc540e659a26616dc7f7f9da1e2.scope\n",
      "               │   └─17706 /coredns -conf /etc/coredns/Corefile\n",
      "               ├─kubepods-burstable-pod53ca5b56a5a7ea86a844b1ad3ead30fe.slice\n",
      "               │ ├─cri-containerd-08d0b974c1ec314f297655bed27bdc23424459ab462113a349dfd1d79b3159f7.scope\n",
      "               │ │ └─19307 kube-controller-manager --allocate-node-cidrs=true --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf --bind-address=0.0.0.0 --client-ca-file=/etc/kubernetes/ssl/ca.crt --cluster-cidr=10.233.64.0/18 --cluster-name=cluster.local --cluster-signing-cert-file=/etc/kubernetes/ssl/ca.crt --cluster-signing-key-file=/etc/kubernetes/ssl/ca.key --configure-cloud-routes=false --controllers=*,bootstrapsigner,tokencleaner --kubeconfig=/etc/kubernetes/controller-manager.conf --leader-elect=true --leader-elect-lease-duration=15s --leader-elect-renew-deadline=10s --node-cidr-mask-size=24 --node-monitor-grace-period=40s --node-monitor-period=5s --profiling=False --requestheader-client-ca-file=/etc/kubernetes/ssl/front-proxy-ca.crt --root-ca-file=/etc/kubernetes/ssl/ca.crt --service-account-private-key-file=/etc/kubernetes/ssl/sa.key --service-cluster-ip-range=10.233.0.0/18 --terminated-pod-gc-threshold=12500 --use-service-account-credentials=true\n",
      "               │ └─cri-containerd-7c9ba38c5a42edce430696c1c2cc74c62301c0a6f1f3e91d94199605e3158989.scope\n",
      "               │   └─13261 /pause\n",
      "               ├─kubepods-burstable-pod7916164a_5bb8_4332_9f7e_b3703746b09d.slice\n",
      "               │ ├─cri-containerd-412ab138c6895c1657d5804b308ee0ad9cfedae36b7f40dabc610e015db02ae0.scope\n",
      "               │ │ └─14962 /pause\n",
      "               │ └─cri-containerd-4d1523c45937f23aea95a2c27a6a4ff29f4b4d4e09cee47c60c00b68c47401ba.scope\n",
      "               │   ├─15669 /usr/local/bin/runsvdir -P /etc/service/enabled\n",
      "               │   ├─15779 runsv felix\n",
      "               │   ├─15780 runsv node-status-reporter\n",
      "               │   ├─15781 runsv allocate-tunnel-addrs\n",
      "               │   ├─15782 runsv monitor-addresses\n",
      "               │   ├─15783 runsv cni\n",
      "               │   ├─15784 calico-node -monitor-token\n",
      "               │   ├─15785 calico-node -monitor-addresses\n",
      "               │   ├─15786 calico-node -allocate-tunnel-addrs\n",
      "               │   ├─15787 calico-node -status-reporter\n",
      "               │   └─15788 calico-node -felix\n",
      "               ├─kubepods-burstable-podb2e1cb3c9d385b5574667e3ff82a9929.slice\n",
      "               │ ├─cri-containerd-06d81e04c2f438516b2dac4cb364652379d443f770c6d1fae73e4b54a97d46bb.scope\n",
      "               │ │ └─13276 /pause\n",
      "               │ └─cri-containerd-31b98c79b5946c3be0467ebfcce91ca14ac8c1fecc0e4545559d787aa3d5acb1.scope\n",
      "               │   └─19268 kube-scheduler --authentication-kubeconfig=/etc/kubernetes/scheduler.conf --authorization-kubeconfig=/etc/kubernetes/scheduler.conf --bind-address=0.0.0.0 --config=/etc/kubernetes/kubescheduler-config.yaml --kubeconfig=/etc/kubernetes/scheduler.conf --leader-elect=true\n",
      "               └─kubepods-burstable-podcae9509a78b479959afe5bbbd967504b.slice\n",
      "                 ├─cri-containerd-b48ab55ead38b28b164cfa0f0c49892d7852db8b40369fe6e799eedf3459a170.scope\n",
      "                 │ └─19031 /pause\n",
      "                 └─cri-containerd-ddba9249da154ad2fe34fa1c6b2d8aa5cfc92ffc3ea4dc41df7053e7a691277d.scope\n",
      "                   └─19064 kube-apiserver --advertise-address=xxx.xxx.235.216 --allow-privileged=true --anonymous-auth=True --apiserver-count=1 --authorization-mode=Node,RBAC --bind-address=0.0.0.0 --client-ca-file=/etc/kubernetes/ssl/ca.crt --default-not-ready-toleration-seconds=300 --default-unreachable-toleration-seconds=300 --enable-admission-plugins=NodeRestriction --enable-aggregator-routing=False --enable-bootstrap-token-auth=true --endpoint-reconciler-type=lease --etcd-cafile=/etc/ssl/etcd/ssl/ca.pem --etcd-certfile=/etc/ssl/etcd/ssl/node-node1.pem --etcd-keyfile=/etc/ssl/etcd/ssl/node-node1-key.pem --etcd-servers=https://xxx.xxx.235.216:2379 --event-ttl=1h0m0s --insecure-port=0 --kubelet-client-certificate=/etc/kubernetes/ssl/apiserver-kubelet-client.crt --kubelet-client-key=/etc/kubernetes/ssl/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalDNS,InternalIP,Hostname,ExternalDNS,ExternalIP --profiling=False --proxy-client-cert-file=/etc/kubernetes/ssl/front-proxy-client.crt --proxy-client-key-file=/etc/kubernetes/ssl/front-proxy-client.key --request-timeout=1m0s --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/etc/kubernetes/ssl/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/etc/kubernetes/ssl/sa.pub --service-account-lookup=True --service-account-signing-key-file=/etc/kubernetes/ssl/sa.key --service-cluster-ip-range=10.233.0.0/18 --service-node-port-range=30000-32767 --storage-backend=etcd3 --tls-cert-file=/etc/kubernetes/ssl/apiserver.crt --tls-private-key-file=/etc/kubernetes/ssl/apiserver.key\n",
      "\n",
      "Jun 06 08:23:30 node1 systemd[1]: Created slice libcontainer container kubepods.slice.\n"
     ]
    }
   ],
   "source": [
    "! ssh node1 systemctl status kubepods.slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d8561b9-2fb3-4153-872d-1f3b0d9aeb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Permanently added 'xxx.xxx.235.216' (ECDSA) to the list of known hosts.\n",
      "CONTAINER    IMAGE    RUNTIME    \n"
     ]
    }
   ],
   "source": [
    "# anything running through containerd ?\n",
    "! ssh node1 ctr c ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac04eded-914d-491a-a25a-01ae52058c36",
   "metadata": {},
   "source": [
    "## etcd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab25ee8d-1d34-4c9c-b4c1-2c9176a74dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "562f5cd7801f1d8b, started, etcd1, https://xxx.xxx.235.216:2380, https://xxx.xxx.235.216:2379, false\n"
     ]
    }
   ],
   "source": [
    "# How many etcd instances are in our cluster ?\n",
    "\n",
    "! ssh -q node1 'etcdctl member list --cert=\"/etc/ssl/etcd/ssl/node-node1.pem\" --key=\"/etc/ssl/etcd/ssl/node-node1-key.pem\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c44ed11-143c-44a5-a611-232c868f5e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/registry/apiextensions.k8s.io/customresourcedefinitions/bgpconfigurations.crd.projectcalico.org\n",
      "\n",
      "/registry/apiextensions.k8s.io/customresourcedefinitions/bgppeers.crd.projectcalico.org\n",
      "\n",
      "/registry/apiextensions.k8s.io/customresourcedefinitions/blockaffinities.crd.projectcalico.org\n",
      "\n",
      "/registry/apiextensions.k8s.io/customresourcedefinitions/caliconodestatuses.crd.projectcalico.org\n",
      "\n",
      "/registry/apiextensions.k8s.io/customresourcedefinitions/clusterinformations.crd.projectcalico.org\n",
      "\n",
      "/registry/apiextensions.k8s.io/customresourcedefinitions/felixconfigurations.crd.projectcalico.org\n",
      "\n",
      "/registry/apiextensions.k8s.io/customresourcedefinitions/globalnetworkpolicies.crd.projectcalico.org\n",
      "\n",
      "/registry/apiextensions.k8s.io/customresourcedefinitions/globalnetworksets.crd.projectcalico.org\n",
      "\n",
      "/registry/apiextensions.k8s.io/customresourcedefinitions/hostendpoints.crd.projectcalico.org\n",
      "\n",
      "/registry/apiextensions.k8s.io/customresourcedefinitions/ipamblocks.crd.projectcalico.org\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List first 20 keys from etcd\n",
    "! ssh -q node1 'etcdctl get --from-key \"\" --keys-only --cert=\"/etc/ssl/etcd/ssl/node-node1.pem\" --key=\"/etc/ssl/etcd/ssl/node-node1-key.pem\" | head -20'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e06028c-a88d-4847-b9f2-e3b81c574b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/registry/services/endpoints/default/kubernetes\n",
      "\n",
      "/registry/services/endpoints/kube-system/coredns\n",
      "\n",
      "/registry/services/specs/default/kubernetes\n",
      "\n",
      "/registry/services/specs/kube-system/coredns\n",
      "\n",
      "/registry/storageclasses/local-path\n",
      "\n",
      "compact_rev_key\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get services\n",
    "! ssh -q node1 'etcdctl get --from-key \"/registry/services\" --keys-only --cert=\"/etc/ssl/etcd/ssl/node-node1.pem\" --key=\"/etc/ssl/etcd/ssl/node-node1-key.pem\" | head -20'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41f2c22e-2b6f-42ee-9f2a-154810575af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/registry/services/endpoints/default/kubernetes\n",
      "k8s\u0000\n",
      "\u000F\n",
      "\u0002v1\u0012\tEndpoints\u0012�\u0002\n",
      "�\u0002\n",
      "\n",
      "kubernetes\u0012\u0000\u001A\u0007default\"\u0000*$34a761b9-fdcf-4664-b7e2-dc7bcac180c52\u00008����\u0006\u0010\u0000Z/\n",
      "'endpointslice.kubernetes.io/skip-mirror\u0012\u0004truez\u0000�\u0001�\u0001\n",
      "\u000Ekube-apiserver\u0012\u0006Update\u001A\u0002v����\u0006\u0010\u0000FieldsV1:d\n",
      "b{\"f:metadata\":{\"f:labels\":{\".\":{},\"f:endpointslice.kubernetes.io/skip-mirror\":{}}},\"f:subsets\":{}}B\u0000\u0012%\n",
      "\u0012\n",
      "\u000Exxx.xxx.235.216\u001A\u0000\u001A\u000F\n",
      "\u0005https\u0010�2\u001A\u0003TCP\u001A\u0000\"\u0000\n"
     ]
    }
   ],
   "source": [
    "# what's in our kubernetes services ?\n",
    "\n",
    "! ssh -q node1 'etcdctl get /registry/services/endpoints/default/kubernetes --cert=\"/etc/ssl/etcd/ssl/node-node1.pem\" --key=\"/etc/ssl/etcd/ssl/node-node1-key.pem\" | head -20'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae45707-049f-49d0-be84-b53931eec1e8",
   "metadata": {},
   "source": [
    "## containerd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a49a08a8-1bf3-4e36-93d7-f9a5b5be9cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID    IMAGE                                       COMMAND                   CREATED        STATUS    PORTS    NAMES\n",
      "03d415712a9f    k8s.gcr.io/kube-proxy:v1.23.7               \"/usr/local/bin/kube…\"    2 hours ago    Up                 k8s://kube-system/kube-proxy-xn9rx/kube-proxy                                         \n",
      "0c12bf085273    k8s.gcr.io/pause:3.3                        \"/pause\"                  2 hours ago    Up                 k8s://kube-system/nodelocaldns-f29kg                                                  \n",
      "143622dbd6d7    k8s.gcr.io/pause:3.3                        \"/pause\"                  2 hours ago    Up                 k8s://kube-system/nginx-proxy-node2                                                   \n",
      "17b03d491dad    k8s.gcr.io/dns/k8s-dns-node-cache:1.21.1    \"/node-cache -locali…\"    2 hours ago    Up                 k8s://kube-system/nodelocaldns-f29kg/node-cache                                       \n",
      "1d23a60e2b99    quay.io/calico/kube-controllers:v3.22.3     \"/usr/bin/kube-contr…\"    2 hours ago    Up                 k8s://kube-system/calico-kube-controllers-6dd874f784-vgt2p/calico-kube-controllers    \n",
      "272c0f3e4f89    k8s.gcr.io/pause:3.3                        \"/pause\"                  2 hours ago    Up                 k8s://kube-system/kube-proxy-xn9rx                                                    \n",
      "37986d4e545e    k8s.gcr.io/pause:3.3                        \"/pause\"                  2 hours ago    Up                 k8s://kube-system/coredns-76b4fb4578-fvkgt                                            \n",
      "71f11adfd533    docker.io/library/nginx:1.21.4              \"/docker-entrypoint.…\"    2 hours ago    Up                 k8s://kube-system/nginx-proxy-node2/nginx-proxy                                       \n",
      "a7e8bbea6cb3    k8s.gcr.io/pause:3.3                        \"/pause\"                  2 hours ago    Up                 k8s://kube-system/calico-kube-controllers-6dd874f784-vgt2p                            \n",
      "b85520a33bd1    quay.io/calico/node:v3.22.3                 \"start_runit\"             2 hours ago    Up                 k8s://kube-system/calico-node-94ldz/calico-node                                       \n",
      "e45fd0d7cdaf    k8s.gcr.io/coredns/coredns:v1.8.6           \"/coredns -conf /etc…\"    2 hours ago    Up                 k8s://kube-system/coredns-76b4fb4578-fvkgt/coredns                                    \n",
      "f93fb4d05e92    k8s.gcr.io/pause:3.3                        \"/pause\"                  2 hours ago    Up                 k8s://kube-system/calico-node-94ldz                                                   \n",
      "\n",
      "Printing just the images: \n",
      "\n",
      "docker.io/library/nginx:1.21.4\n",
      "k8s.gcr.io/coredns/coredns:v1.8.6\n",
      "k8s.gcr.io/dns/k8s-dns-node-cache:1.21.1\n",
      "k8s.gcr.io/kube-proxy:v1.23.7\n",
      "k8s.gcr.io/pause:3.3\n",
      "quay.io/calico/kube-controllers:v3.22.3\n",
      "quay.io/calico/node:v3.22.3\n"
     ]
    }
   ],
   "source": [
    "# Which containers are running on node2 ?\n",
    "! ssh -q node2 \"nerdctl ps\"\n",
    "! echo \"\\nPrinting just the images: \\n\"\n",
    "! ssh -q node2 \"nerdctl ps | tail -n +2 | tr -s ' ' | cut -d' ' -f2 | sort | uniq\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ace6cd71-3b8f-42a2-b199-ab32cbac6d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID    IMAGE                                               COMMAND                   CREATED        STATUS    PORTS    NAMES\n",
      "1210821f8181    k8s.gcr.io/dns/k8s-dns-node-cache:1.21.1            \"/node-cache -locali…\"    2 hours ago    Up                 k8s://kube-system/nodelocaldns-s728x/node-cache                                            \n",
      "4b9c1c7a61af    k8s.gcr.io/pause:3.3                                \"/pause\"                  2 hours ago    Up                 k8s://kube-system/calico-node-h47zr                                                        \n",
      "56abc426aabc    k8s.gcr.io/pause:3.3                                \"/pause\"                  2 hours ago    Up                 k8s://kube-system/kube-proxy-b9gh2                                                         \n",
      "8a92fd9a6409    docker.io/library/nginx:1.21.4                      \"/docker-entrypoint.…\"    2 hours ago    Up                 k8s://kube-system/nginx-proxy-node3/nginx-proxy                                            \n",
      "8b16e9831009    quay.io/calico/node:v3.22.3                         \"start_runit\"             2 hours ago    Up                 k8s://kube-system/calico-node-h47zr/calico-node                                            \n",
      "ae0dade85a32    k8s.gcr.io/pause:3.3                                \"/pause\"                  2 hours ago    Up                 k8s://local-path-storage/local-path-provisioner-6957789775-h7rss                           \n",
      "bb9c52b37332    k8s.gcr.io/kube-proxy:v1.23.7                       \"/usr/local/bin/kube…\"    2 hours ago    Up                 k8s://kube-system/kube-proxy-b9gh2/kube-proxy                                              \n",
      "c5dfa941c65e    k8s.gcr.io/pause:3.3                                \"/pause\"                  2 hours ago    Up                 k8s://kube-system/nodelocaldns-s728x                                                       \n",
      "d20b73a79052    docker.io/rancher/local-path-provisioner:v0.0.21    \"local-path-provisio…\"    2 hours ago    Up                 k8s://local-path-storage/local-path-provisioner-6957789775-h7rss/local-path-provisioner    \n",
      "e92741c303fd    k8s.gcr.io/pause:3.3                                \"/pause\"                  2 hours ago    Up                 k8s://kube-system/nginx-proxy-node3                                                        \n",
      "\n",
      "Printing just the images: \n",
      "\n",
      "docker.io/library/nginx:1.21.4\n",
      "docker.io/rancher/local-path-provisioner:v0.0.21\n",
      "k8s.gcr.io/dns/k8s-dns-node-cache:1.21.1\n",
      "k8s.gcr.io/kube-proxy:v1.23.7\n",
      "k8s.gcr.io/pause:3.3\n",
      "quay.io/calico/node:v3.22.3\n"
     ]
    }
   ],
   "source": [
    "# Which containers are running on node3 ?\n",
    "! ssh -q node3 \"nerdctl ps\"\n",
    "! echo \"\\nPrinting just the images: \\n\"\n",
    "! ssh -q node3 \"nerdctl ps | tail -n +2 | tr -s ' ' | cut -d' ' -f2 | sort | uniq\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13aef0dd-0f6e-49fd-ad42-8cdb8ea98c16",
   "metadata": {},
   "source": [
    "* quay.io/calico/kube-controllers is only running on node 2\n",
    "* k8s.gcr.io/coredns/coredns is only running on node 2\n",
    "* docker.io/rancher/local-path-provisioner is only running on node 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b014de91-9dc7-440f-b678-a1a4e318e03e",
   "metadata": {},
   "source": [
    "## metrics server\n",
    "\n",
    "**The metrics pipline needs to be installed in the cluster**\n",
    "\n",
    "See [https://kubernetes.io/docs/tasks/debug/debug-cluster/resource-metrics-pipeline/](https://kubernetes.io/docs/tasks/debug/debug-cluster/resource-metrics-pipeline/)\n",
    "\n",
    "To activate with kubespray make sure `metrics_server_enabled: true` is set in addons.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a6bc8fd-a4c5-4e66-9ad6-4b6759af1128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME    CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   \n",
      "node1   330m         18%    1608Mi          49%       \n"
     ]
    }
   ],
   "source": [
    "! kubectl top node node1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "38c69648-1356-435b-9e53-58c199c34b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                              CPU(cores)   MEMORY(bytes)   \n",
      "metrics-server-5c8c77d7b8-xz79p   7m           17Mi            \n"
     ]
    }
   ],
   "source": [
    "# What's the top of the metrics-server ?\n",
    "! kubectl top po $(kubectl get po -n kube-system | grep metrics | tr -s ' ' | cut -d' ' -f1) -n kube-system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6be9a9f-0c88-40ed-86e9-45b25579a855",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}